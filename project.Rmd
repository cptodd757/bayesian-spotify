---
title: "Bayesian Spotify"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Modeling Song Popularity with Bayesian Linear Regression on Spotify Data

## STA360 Final Project

## Charlie Todd and Alex Balfanz

### Introduction

  Over the course of the past decade, music streaming services such as Spotify have changed the landscape of the music industry.  For consumers, all kinds of music are now much more accessible than ever; for artists, then, the potential for exposure and recognition is likewise at an all-time high.  So, an investigation into the ingredients for creating a popular song has massive implications, especially for artists, if they want to better understand how to increase their chances of attracting attention and producing hits.  Furthermore, Spotify itself, along with other companies in the entertainment industry, could capitalize off of targeting songs with high predicted popularity in hopes of fueling marketing campaigns or other strategies.  The idea of song popularity also piques the interest of everyday consumers of music, such as ourselves, as evidenced by the online success of marketing programs like Spotify Wrapped and the prevalence of music as a topic in common conversation.
	
	Therefore, the goals for this project are to both explain and predict the popularity of songs on Spotify, using a Bayesian generalized linear model.  Our dataset has been constructed using the Spotify Web API, which exposes valuable information about its catalogue of songs, to be explained later in greater detail.  The model will predict the “popularity” score, an advanced metric computed by Spotify’s own proprietary algorithms, based on a number of other continuous and categorical predictor variables available through the API.  We will first construct a full model based on all predictor variables deemed to be relevant beforehand, interpret the results of this model, and then use backwards selection to generate the most succinct model for predicting the popularity of previously unseen songs.

### Data

	The dataset used in this investigation has been compiled using a Python script which communicates with the Spotify Web API.  It consists of 412 songs released in the 2019 calendar year by top performers in the rap, pop, and electronic dance music (EDM) genres.  We chose to select songs only release in 2019 due to the most prominent challenge regarding our data: Spotify does not allow access to raw metrics such as number of recent or total streams, instead offering their own popularity metric, which is based on release date in some undisclosed way.  So, to eliminate any unwanted and inexplicable variation in the response based on Spotify’s algorithms, we have kept the release year constant, selecting the most recent full year.  We selected rap, pop, and EDM because these are some of the most prominent genres today, and we wanted to observe potential interaction effects that govern the popularity of songs in these particular genres.  
	
	The dataset consists of 19 variables which we have categorized as metadata, basic features, and advanced features.  The metadata consists of straightforward information which is not used in the model, such as song title and artist.  The simple features contain other straightforward quantities like duration, and strictly defined musical quantities such as tempo, some of which are computed with Spotify’s algorithms.  The advanced features are some more abstract quantities ranging from 0 to 1 such as “danceability” and “speechiness”, which are all computed by Spotify’s algorithms and made available through the API. 
	
	More information about Spotify Audio Features can be found here: https://developer.spotify.com/documentation/web-api/reference/tracks/get-several-audio-features/


Metadata:

- Title: title of the song
- Artist: one of the artists contributing to a song
- Album: the album on Spotify which features this song (includes wrappers for single releases, as well as remixes)
- Id: unique ID number for the song

Basic predictors:

- Genre: the overarching genre to which the song belongs (rap, pop, or EDM)
- Key: the key signature of the song (0 through 11, with 0 corresponding to C), with Mode
- Mode: whether or not a song is in a “major” (1) or “minor” (0) key 
- Time signature: the time signature of the song; e.g., 2, 3, 4, or 0, if there is no discernible one
- Duration: duration of the song in milliseconds.
- Tempo: the song’s number of beats per minute 
- Loudness: the average decibel level of the song

Advanced features (calculated by Spotify’s algorithms behind-the-scenes:

- Acousticness: how much a song sounds like it was produced with real, non-electronic instruments
- Danceability: how suitable a song is for dancing, based on rhythms and tempo stability
- Energy: how much energy and excitement a song has, as opposed to calmness
- Instrumentalness: how much a song doesn’t have vocals in it
- Liveness: how confident Spotify is that the song is from a live concert
- Popularity: a measure based on total number of plays and recency of those plays, among other things
- Speechiness: how much spoken word the song contains
- Valence: how positive and happy a song sounds, contrasted with sad or negative

```{r echo=FALSE}
library(tidyverse)
library(ggplot2)
library(rstan)
library(rstanarm)
library(pracma)
```


## Analysis

```{r}
data <- read_csv("data/Travis Scott_Drake_DaBaby_Kanye West_Offset_Lil Skies_Ed Sheeran_Ariana Grande_Billie Eilish_Jonas Brothers_Camila Cabello_Harry Styles_ILLENIUM_Diplo_Tiësto_Marshmello_Flume_Avicii_2019.csv")
```



Once we have loaded the dataset, our first step is to categorize each song into the genre of pop, rap, or EDM, depending on who the artist is.  We chose artists that are at the top of their respective genres and have recently released albums.  

We also convert some of our categorical variables into factor variables, so that our graphing and modeling packages treat them as such.


```{r}

data <- data %>% mutate(
  genre = case_when(
    artist == "Travis Scott" | artist == "Drake" | artist == "DaBaby" | artist == "Kanye West" | artist == "Offset" | artist == "Lil Skies" ~ "rap", 
    artist == "Ed Sheeran" | artist == "Ariana Grande" | artist == "Billie Eilish" | artist == "Jonas Brothers" | artist == "Camila Cabello" | artist == "Harry Styles" ~ "pop", 
    artist == "ILLENIUM" | artist == "Diplo" | artist == "Tiësto" | artist == "Marshmello" | artist == "Flume" | artist == "Avicii" ~ "edm"
  ),
  album = as.factor(album),
  artist = as.factor(artist),
  genre = relevel(as.factor(genre), "pop"),
  key = as.factor(key),
  mode = as.factor(mode),
  time_signature = as.factor(time_signature),
  title = as.factor(title),
)
```

```{r echo=FALSE}
data %>% select("artist") %>% unique()
```

```{r echo=FALSE}
data %>% filter(is.na(genre))
```
```{r echo=FALSE}
data %>% select(genre) %>% unique()
```

Our next step is to rescale our response variable and some of our predictor variables.  We rescale our response variable, popularity, because its support originally ranges from 0 to 100 (inclusive), but we want its support to range from 0 to 1 inclusive, so that it can be modeled as a Beta random variable.  We rescale some of our predictor variables so that all of them have a similar range, which will prevent our model coefficients from ultimately being uninterprably small or large.  For example, duration (measured at first in milliseconds) originally is on the order of hundred-thousands, and Spotify metrics like danceability are bounded between 0 and 1.  So, we chose to scale these Spotify metrics up by 100, and scale duration from milliseconds down to seconds.  

# TODO: MEAN CENTERING

```{r}
data_norm <- data %>% mutate(
  popularity = popularity / 100,
  duration_ms = duration_ms / 1000,
  acousticness = (acousticness) * 100,
  danceability = danceability * 100,
  energy = energy * 100,
  instrumentalness = (instrumentalness) * 100,
  liveness = (liveness) * 100,
  speechiness = (speechiness) * 100,
  valence = valence * 100,
  # tempo,
  # loudness,
)

# mean center (so we can interpret the intercept)
data_norm <- data_norm %>% mutate(
  duration_ms = duration_ms - mean(duration_ms),
  acousticness = acousticness - mean(acousticness),
  danceability = danceability - mean(danceability),
  energy = energy - mean(energy),
  instrumentalness = instrumentalness - mean(instrumentalness),
  liveness = liveness - mean(liveness),
  speechiness = speechiness - mean(speechiness),
  valence = valence - mean(valence),
  tempo = tempo - mean(tempo),
  loudness = loudness - mean(loudness)
)
```

Next, we perform univariate analysis by examining the distributions of the response and predictor variables.

```{r}
data %>%
  keep(is.numeric) %>% 
  select(-X1) %>%
  gather() %>% 
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_histogram() +
    ggtitle("Distributions of Continuous Predictor Variables")

data %>%
  keep(is.factor) %>% 
  select(-album, -title) %>%
  gather() %>% 
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_bar() +
    ggtitle("Distributions of Categorical Predictor Variables")
```

Many of the continuous predictor variables follow approximately normal distributions, such as duration and danceability.  Others, like liveness, acousticness, instrumentalness, and speechiness, appear much more skewed right--this makes sense because the tracks we selected are songs from studio albums, in genres that rely heavily on electronic music production.  As a result, before fitting our model, we will log-transform these variables to create more normal-looking distributions.  Our response variable, popularity, is unimodally distributed over the domain [0, 1], so we have chosen to use a Beta distribution to model it.

Three of our four categorical variables (artist, genre, and mode) appear relatively uniformly distributed, meaning we have a good balance of songs with respect to those categories.  Time signature, on the other hand, takes a value of 4 for almost all of the observations (this is consistent with all music in general), so it will not be useful to include as a predictor in the model.

Next, we observe the individual correlations between each predictor variable and popularity.

```{r}

dontgraph = c('X1','popularity','id','album','type','title')

for (i in colnames(data))
{
  if (i %in% dontgraph)
  {
  }
  else if (!is.numeric(data[[i]]))
  {
    print(ggplot(data,aes(data[[i]],popularity))+
      geom_boxplot()+
      labs(x=i,y="Popularity",title=paste("Popularity vs.",i)))
  }
  else 
  {
    print(ggplot(data,aes(data[[i]],popularity))+
      geom_point()+
      labs(x=i,y="Popularity",title=paste("Popularity vs.",i)))
  }
}
```

Based on these plots, there appear to be some weak correlations between popularity and predictor variables, such as the positive correlation with danceability and the negative correlation with energy.  There also appears to be a difference in overall popularity between edm and the other two genres.  Although the correlations appear small in these graphs, we predict that they will be non-trivial, and that interesting interaction effects between genre and other variables will also account for variation in popularity.  

# Model

Next, we will construct a model for our data. 

```{r echo = T, results = 'hide'}
cols <- cbind(
  "genre",
  "acousticness",
  "danceability",
  "duration_ms",
  "energy",
  "instrumentalness",
  "liveness",
  "loudness",
  "speechiness",
  "tempo",
  "valence",
  "mode",
  "genre * energy",
  "genre * tempo",
  "genre * speechiness",
  "genre * duration_ms",
  "genre * loudness")

form <- as.formula(paste0("popularity ~ ", paste(cols, collapse = "+")))
fit <- stan_betareg(form,

  data = data_norm, 
  link = "logit", 
  algorithm = "sampling"
)
```

```{r}
summary(fit, digits = 5, probs = c(0.05, 0.95))
```

# Results

So, now that our model has been fit with `stan_betareg` (using a `logit` link), we can interpret the coefficients in the context of predicting popularity of a song. Along with these coefficients, we have computed a 90% confidence interval so that we may determine which predictors, in this model, appear to be significant in predicting the popularity of a song. These are: 

- genre, 
- duration (& pop baseline), 
- energy (& pop baseline), 
- liveness, 
- speechiness (& pop baseline), 
- interaction (genre=rap) * energy, 
- interaction (genre=edm) * speechiness, 
- interaction (genre=rap) * speechiness,
- interaction (genre=edm) * duration
- interaction (genre=rap) * duration
- interaction (genre=edm) * loudness

It's important to keep in mind that these coefficients are transformed through our `logit` link, so we need to take the `inv.logit` (found in library `gtools`) to calculate the actual per-unit change in `popularity`. The sign/direction of popularity change, however, remains the same whether transformed or not.


First, let's take a look at our trace plots to see if we're sampling fairly efficiently, as opposed to getting stuck in any local optimum.

```{r}
stan_trace(fit, pars=c("(Intercept)",
"genreedm",
"genrerap",
"acousticness",
"danceability",
"duration_ms",
"energy",
"instrumentalness",
"liveness",
"loudness",
"speechiness",
"tempo",
"valence",
"mode1",
"genreedm:energy",
"genrerap:energy",
"genreedm:tempo",
"genrerap:tempo",
"genreedm:speechiness",
"genrerap:speechiness",
"genreedm:duration_ms",
"genrerap:duration_ms",
"genreedm:loudness",
"genrerap:loudness",
"(phi)"))
```

Everything here looks fairly random.


Next, we'll do a posterior predictive check to see how sampled random predicted popularities line up with our real popularities.

```{r}
pp_check(fit)
```

Nothing too surprising here. We're predicting with a beta distribution, and it looks similiar, but of course predicting popularity is difficult.

###  Key Findings

- Since we mean centered our data, we can interpret our intercepts as the popularity metric of a song with average attributes for all predictors. For a pop song, our mean popularity is `inv.logit(0.73613) = 67.6%`, a rap song is  `inv.logit(0.73613 - 0.20453) = 62.9%`, and an edm song is `inv.logit(0.73613 -0.63032) = 52.6%`. And this makes sense. Pop is called pop for a reason, it's more popular. And as of right now, it seems like, on Spotify at least, rap music is more popular than edm on average. But of course, this is also dependent on our dataset and which artists we choose. In an ideal world, we would have every artist under all three genres. This is also why popularity is so high, even on all three,.. we've selectively chosen popular artists that most people will know. Thus, our model is only good at predicting popularity of songs of artists similiar to the ones we have chosen. Otherwise, we're outside of the domain.

- Loudness itself is not a significant predictor, until it is paired with edm music. The coefficient is positive, which implies that the louder the song (in the context of edm genre), the more popular the song may be.

- Speechiness, at a baseline, has a positive coefficient, which implies that the speechier a song is, the more popular it may be. However, this is not the case when interaction between genre is added. The mean coefficiet for edm (0.01351 - 0.03057 < 0) and rap (0.01351 - 0.01509 < 0) both contribute to less popularity. Thus, we can conclude that speechiness in pop music is correlated to increased popularity, whereas in edm music increased speechiness is correlated to decreased popularity. This is also the case in rap music, which is somewhat surprising, as rap music is typically fairly lyrically heavy, but since the value is fairly close to 0 (0.01351 - 0.01509) all we really know is that speechiness in rap music perhaps doesn't matter as much to the listener, and therefore has little effect on popularity.

- Duration, at the baseline, has a positive coefficient, which implies that the longer a song is, the more popular it may be. This is only the case when looking at the `pop` genre, however. When we look at `rap` and `edm`, we find that the coefficient is negative, thus in these genres, listeners seem to lose interest as the song becomes longer (i.e. lower popularity).

- Although mode is not a significant predictor, it is definitely weighted to one side. A mode of 0 implies the song is in minor key, while a mode of 1 implies the song is in major key. And the weighting is towards... minor! Thus, all else equal, our model will predict a song to have higher popularity if it is in minor key. We can infer, then, that people prefer songs that sound sad. But again, this is only a slight, and seemingly insignificant predictive change, according to quantiles. 

# Results

Predicting song popularity is difficult...

Talk about how model only good at predicting data inside the data domain (e.g. popular artists).